# ì„œìš¸ì‹œ LLMOps íŒŒì´í”„ë¼ì¸

ì„œìš¸ì‹œ AI Agentë¥¼ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ LLM í•™ìŠµ ë° ê²€ì¦ íŒŒì´í”„ë¼ì¸

## ğŸ“‹ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” ì„œìš¸ì‹œë¥¼ ìœ„í•œ ì‘ì€ LLM(Large Language Model) ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤. [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ëª¨ë¸ í•™ìŠµì„ ì œê³µí•˜ê³ , FastAPI ê¸°ë°˜ì˜ ì¶”ë¡  ì„œë²„ë¥¼ í†µí•´ API ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### ğŸ¯ ì£¼ìš” íŠ¹ì§•

- **Axolotl ê¸°ë°˜ í•™ìŠµ**: ìµœì‹  LoRA íŒŒì¸íŠœë‹ ê¸°ìˆ  í™œìš©
- **ì‘ì€ ëª¨ë¸ ìµœì í™”**: ì œí•œëœ ìì›ìœ¼ë¡œë„ íš¨ê³¼ì ì¸ í•™ìŠµ
- **FastAPI ì„œë²„**: RESTful APIë¥¼ í†µí•œ ì¶”ë¡  ì„œë¹„ìŠ¤
- **Docker ì§€ì›**: ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ë°°í¬ ë° ì‹¤í–‰
- **ì„œìš¸ì‹œ íŠ¹í™”**: ì„œìš¸ì‹œ ê´€ë ¨ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ì „ êµ¬ì„±

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì „ì œ ì¡°ê±´

- Python 3.10+
- NVIDIA GPU (ê¶Œì¥)
- Docker & Docker Compose
- 8GB+ GPU ë©”ëª¨ë¦¬ (ê¶Œì¥)

### 1. í”„ë¡œì íŠ¸ í´ë¡ 

```bash
git clone https://github.com/your-repo/seoul-llmops.git
cd seoul-llmops
```

### 2. í™˜ê²½ ì„¤ì •

#### ì˜µì…˜ A: Docker ì‚¬ìš© (ê¶Œì¥)

```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker-compose build

# API ì„œë²„ ì‹¤í–‰
docker-compose --profile api up -d

# í•™ìŠµ ì‹¤í–‰
docker-compose --profile training up
```

#### ì˜µì…˜ B: ë¡œì»¬ í™˜ê²½

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# accelerate ì„¤ì •
accelerate config
```

### 3. ëª¨ë¸ í•™ìŠµ

```bash
# í•™ìŠµ ì‹¤í–‰
python scripts/train_model.py --config config/seoul_llm_config.yml

# íŠ¹ì • ë‹¨ê³„ë§Œ ì‹¤í–‰
python scripts/train_model.py --skip-preprocess  # ì „ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°
python scripts/train_model.py --skip-training    # í•™ìŠµ ê±´ë„ˆë›°ê¸°
```

### 4. API ì„œë²„ ì‹¤í–‰

```bash
# ë¡œì»¬ ì‹¤í–‰
python api/server.py --host 0.0.0.0 --port 8000

# íŠ¹ì • ëª¨ë¸ ê²½ë¡œ ì§€ì •
python api/server.py --model-path ./models/seoul-llm-lora/merged
```

### 5. API í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ ì²´í¬
curl http://localhost:8000/health

# í…ìŠ¤íŠ¸ ìƒì„±
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "ì„œìš¸ì‹œì˜ ì¸êµ¬ëŠ” ì–¼ë§ˆì¸ê°€ìš”?"}'

# ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python scripts/example_usage.py
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
seoul-llmops/
â”œâ”€â”€ api/                    # API ì„œë²„
â”‚   â””â”€â”€ server.py          # FastAPI ì„œë²„
â”œâ”€â”€ config/                # ì„¤ì • íŒŒì¼
â”‚   â””â”€â”€ seoul_llm_config.yml
â”œâ”€â”€ data/                  # ë°ì´í„°ì…‹
â”‚   â””â”€â”€ seoul_dataset.jsonl
â”œâ”€â”€ scripts/               # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ train_model.py     # í•™ìŠµ íŒŒì´í”„ë¼ì¸
â”‚   â””â”€â”€ example_usage.py   # API ì‚¬ìš© ì˜ˆì œ
â”œâ”€â”€ models/                # ëª¨ë¸ ì €ì¥ì†Œ
â”œâ”€â”€ logs/                  # ë¡œê·¸ íŒŒì¼
â”œâ”€â”€ Dockerfile             # Docker ì„¤ì •
â”œâ”€â”€ docker-compose.yml     # Docker Compose
â”œâ”€â”€ requirements.txt       # Python ì˜ì¡´ì„±
â””â”€â”€ README.md             # ë¬¸ì„œ
```

## âš™ï¸ ì„¤ì •

### í•™ìŠµ ì„¤ì • (config/seoul_llm_config.yml)

ì£¼ìš” ì„¤ì • í•­ëª©:

```yaml
# ë² ì´ìŠ¤ ëª¨ë¸
base_model: microsoft/DialoGPT-small

# LoRA ì„¤ì •
adapter: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# í•™ìŠµ íŒŒë¼ë¯¸í„°
micro_batch_size: 4
gradient_accumulation_steps: 2
num_epochs: 3
learning_rate: 0.0002

# ë°ì´í„°ì…‹
datasets:
  - path: data/seoul_dataset.jsonl
    ds_type: json
    type:
      system_prompt: "ì„œìš¸ì‹œ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ..."
```

### í™˜ê²½ ë³€ìˆ˜

```bash
# ëª¨ë¸ ê²½ë¡œ
export MODEL_PATH="./models/seoul-llm-lora/merged"

# LoRA ì–´ëŒ‘í„° ê²½ë¡œ
export LORA_PATH="./models/seoul-llm-lora"

# GPU ì„¤ì •
export CUDA_VISIBLE_DEVICES=0
```

## ğŸ”§ ì‚¬ìš©ë²•

### 1. ë°ì´í„°ì…‹ ì¤€ë¹„

JSONL í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì¤€ë¹„:

```json
{"instruction": "ì§ˆë¬¸", "input": "ì¶”ê°€ ì…ë ¥(ì„ íƒ)", "output": "ë‹µë³€"}
```

### 2. ëª¨ë¸ í•™ìŠµ

```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
python scripts/train_model.py

# ë‹¨ê³„ë³„ ì‹¤í–‰
python scripts/train_model.py --skip-preprocess
python scripts/train_model.py --skip-training --skip-merge
```

### 3. API ì‚¬ìš©

#### í…ìŠ¤íŠ¸ ìƒì„±

```python
import requests

response = requests.post("http://localhost:8000/generate", json={
    "prompt": "ì„œìš¸ì‹œì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
    "max_new_tokens": 100,
    "temperature": 0.7
})

print(response.json()["generated_text"])
```

#### ì±„íŒ… ì¸í„°í˜ì´ìŠ¤

```python
response = requests.post("http://localhost:8000/chat", json={
    "prompt": "ì•ˆë…•í•˜ì„¸ìš”"
})

print(response.json()["response"])
```

## ğŸ³ Docker ì‚¬ìš©ë²•

### í”„ë¡œí•„ë³„ ì‹¤í–‰

```bash
# API ì„œë²„ë§Œ ì‹¤í–‰
docker-compose --profile api up -d

# í•™ìŠµ í™˜ê²½ ì‹¤í–‰
docker-compose --profile training up

# ê°œë°œ í™˜ê²½ (Jupyter í¬í•¨)
docker-compose --profile dev up -d
```

### ë³¼ë¥¨ ê´€ë¦¬

```bash
# ëª¨ë¸ ë°ì´í„° ë°±ì—…
docker run --rm -v seoul-llmops_huggingface_cache:/data alpine tar czf /backup.tar.gz /data

# ë¡œê·¸ í™•ì¸
docker-compose logs seoul-llm-api
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### GPU ë©”ëª¨ë¦¬ ìµœì í™”

```yaml
# config/seoul_llm_config.yml
fp16: true                    # ë°˜ì •ë°€ë„ ì‚¬ìš©
gradient_checkpointing: true  # ë©”ëª¨ë¦¬ ì ˆì•½
micro_batch_size: 2          # ë°°ì¹˜ í¬ê¸° ê°ì†Œ
```

### ì¶”ë¡  ìµœì í™”

```python
# API ì„œë²„ì—ì„œ ìºì‹± í™œìš©
generation_config = GenerationConfig(
    use_cache=True,
    do_sample=True,
    temperature=0.7
)
```

## ğŸ” ëª¨ë‹ˆí„°ë§

### ë¡œê·¸ í™•ì¸

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸
tail -f logs/training.log

# Docker ë¡œê·¸
docker-compose logs -f seoul-llm-api
```

### Weights & Biases (ì„ íƒì‚¬í•­)

```yaml
# config/seoul_llm_config.yml
wandb_mode: online
wandb_project: seoul-llm-training
wandb_entity: your-team
```

## ğŸ› ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œ

1. **CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±**
   ```yaml
   micro_batch_size: 1
   gradient_accumulation_steps: 4
   fp16: true
   ```

2. **ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨**
   ```bash
   # ê¶Œí•œ í™•ì¸
   chmod -R 755 models/
   
   # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
   df -h
   ```

3. **API ì„œë²„ ì—°ê²° ì‹¤íŒ¨**
   ```bash
   # í¬íŠ¸ í™•ì¸
   netstat -tulpn | grep 8000
   
   # ë°©í™”ë²½ ì„¤ì •
   sudo ufw allow 8000
   ```

### ë””ë²„ê¹…

```bash
# ìƒì„¸ ë¡œê·¸ í™œì„±í™”
export PYTHONPATH=$PWD
python -u scripts/train_model.py --config config/seoul_llm_config.yml

# API ë””ë²„ê·¸ ëª¨ë“œ
python api/server.py --reload
```

## ğŸ“ˆ í™•ì¥ ê°€ëŠ¥ì„±

### ë” í° ëª¨ë¸ ì‚¬ìš©

```yaml
# config/seoul_llm_config.yml
base_model: microsoft/DialoGPT-medium  # ë˜ëŠ” large
lora_r: 32
lora_alpha: 64
```

### ë‹¤ì¤‘ GPU í•™ìŠµ

```yaml
# DeepSpeed ì„¤ì •
deepspeed: deepspeed/zero2.json
```

### í”„ë¡œë•ì…˜ ë°°í¬

```bash
# Gunicorn ì‚¬ìš©
gunicorn api.server:app -w 4 -k uvicorn.workers.UvicornWorker
```

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” Apache 2.0 ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

## ğŸ“ ì§€ì›

- ì´ìŠˆ ë¦¬í¬íŠ¸: [GitHub Issues](https://github.com/your-repo/seoul-llmops/issues)
- ë¬¸ì˜: your-email@seoul.go.kr

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Axolotl Documentation](https://docs.axolotl.ai/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [Transformers Library](https://huggingface.co/docs/transformers/)
